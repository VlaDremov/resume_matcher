# Cluster-Driven Resume Pipeline Refactor

This ExecPlan is a living document. The sections `Progress`, `Surprises & Discoveries`, `Decision Log`, and `Outcomes & Retrospective` must be kept up to date as work proceeds.

The repository includes `PLANS.md` at the root. This ExecPlan must be maintained in accordance with `PLANS.md`.

## Purpose / Big Picture

After this change, a user can cluster the job descriptions in `vacancies/` into N clusters, persist a reusable JSON artifact, generate one resume variant per cluster using the artifact’s keywords, and match any new job description directly to the best cluster. Cluster output should no longer include irrelevant keywords (for example, pronouns like "you"). The CLI, backend API, and frontend UI must reflect dynamic clusters rather than the fixed three themes. LinkedIn scraping functionality and dependencies are removed. The behavior is visible by running `python main.py cluster-vacancies` to create `output/vacancy_clusters.json`, running `python main.py generate` to create `output/resume_<cluster>.tex`, and calling `python main.py tailor` or `/api/analyze` to see cluster-based matches.

## Progress

- [x] (2026-01-11 17:03Z) Reviewed clustering, resume generation, matching, backend schemas, frontend rendering, and LinkedIn scraper references.
- [x] (2026-01-11 19:10Z) Implemented cluster artifact schema and keyword noise filtering in vacancy clustering.
- [x] (2026-01-11 19:25Z) Generated resume variants from cluster artifacts and updated GPT rewrite prompts to use cluster keywords.
- [x] (2026-01-11 19:40Z) Replaced matching and keyword categorization with cluster-based logic and updated API/frontend types.
- [x] (2026-01-11 19:55Z) Removed LinkedIn scraper code and cleaned unused legacy theme/matcher code; updated docs and dependencies.

## Surprises & Discoveries

- Observation: `PLANS.md` was missing despite instructions; created it to store this ExecPlan.
  Evidence: `rg --files -g 'PLANS.md'` returned no matches before creation.
- Observation: Backend schemas and frontend UI were hard-coded to three categories, which required dynamic handling.
  Evidence: `backend/schemas.py`, `backend/services.py`, `frontend/src/api.ts`, `frontend/src/components/ScoreDisplay.tsx`.

## Decision Log

- Decision: Create and use `PLANS.md` as the governing ExecPlan specification since it was missing.
  Rationale: Required by the instructions and needed for ongoing plan updates.
  Date/Author: 2026-01-11, Codex.
- Decision: Persist the clustering result to `output/vacancy_clusters.json` with a versioned schema and a vacancy file signature.
  Rationale: A stable artifact is required for reuse and for detecting stale cluster data.
  Date/Author: 2026-01-11, Codex.
- Decision: Match new vacancies to clusters using embeddings of cluster profile text rather than legacy resume variant embeddings.
  Rationale: The user explicitly wants matching against the clusters created in step 1.
  Date/Author: 2026-01-11, Codex.

## Outcomes & Retrospective

All milestones complete. Clustering now filters noise and writes a versioned cluster artifact, resume generation is cluster-driven with GPT prompts using cluster keywords, matching and keyword categorization are cluster-based with dynamic API/frontend handling, and LinkedIn scraping code/dependencies were removed while docs were updated. Remaining work is limited to running the validation commands in this plan as desired.

## Context and Orientation

The CLI entry point is `main.py`, which wires commands to logic under `src/`. The current clustering pipeline is `src/vacancy_clustering.py` and uses TF-IDF, optional LLM enrichment, and embeddings to cluster `vacancies/*.txt`. Resume variants are generated by `src/resume_generator.py`, which now consumes cluster artifacts rather than fixed themes. Matching is done by `src/cluster_matcher.py` and is used by `backend/services.py` to choose a resume variant. The backend response schema in `backend/schemas.py` and frontend UI in `frontend/src/components/ScoreDisplay.tsx` now allow dynamic categories. LinkedIn scraping lived in `deprecated/linkedin_scraper.py` and depended on Selenium, both removed.

Define the new terms used in this plan. A "cluster artifact" is a JSON file that records the cluster list, cluster names, keywords, and metadata produced from `vacancies/`. A "cluster profile text" is a plain-text summary built from a cluster’s name, summary, and keywords, used to compare new job descriptions to clusters. An "embedding" is a numeric vector representation of text that allows similarity comparisons; the repo already computes embeddings with local SentenceTransformers and optional OpenAI embeddings.

## Plan of Work

First, extend `src/vacancy_clustering.py` to filter noisy keywords (pronouns and generic words) using both local heuristics and LLM guidance. Update the LLM keyword enrichment to explicitly mark and drop irrelevant keywords. Add a new `src/cluster_artifacts.py` module that converts `ClusterResult` into a versioned JSON artifact, includes a vacancy file signature, and generates cluster profile text. Update the `cluster-vacancies` CLI in `main.py` to always write the artifact (defaulting to `output/vacancy_clusters.json`) while preserving the existing cache behavior.

Next, refactor resume generation to be cluster-driven. Replace `get_resume_themes()` usage with a new theme builder that reads the cluster artifact and produces per-cluster theme configs (name, keywords, skills priority, summary template). Update `src/resume_generator.py` and `src/bullet_rewriter.py` to consume cluster keyword lists instead of fixed taxonomy categories. Ensure resume filenames are based on the cluster slug and that summaries and skill ordering use the cluster’s defining keywords.

Then, implement a cluster matcher. Add `src/cluster_matcher.py` to load the artifact, build embeddings for cluster profile text, and match a new job description to the best cluster. Update `main.py tailor` to use this matcher and select the variant by cluster slug. Update `backend/services.py` to use the cluster matcher for `best_variant`, and update keyword categorization to use cluster keyword sets instead of the static taxonomy. Modify `backend/schemas.py` and `frontend/src/api.ts` to allow dynamic categories, and update `frontend/src/components/ScoreDisplay.tsx` to render categories from response data rather than a hard-coded list.

Finally, remove unused and redundant code. Delete the LinkedIn scraper (`deprecated/linkedin_scraper.py`), remove the `scrape` CLI command from `main.py`, and drop Selenium dependencies from `requirements.txt`. Remove or refactor legacy functions in `src/keyword_engine.py` and `src/semantic_matcher.py` that are no longer used after the cluster-based flow is in place. Update `README.md` to document the new cluster artifact flow and remove LinkedIn scraper references.

## Concrete Steps

From the repository root:

    python main.py cluster-vacancies --clusters 4 --output output/vacancy_clusters.json

Expected: `output/vacancy_clusters.json` exists with a `schema_version`, `num_clusters`, and `clusters` list.

    python main.py generate --clusters-artifact output/vacancy_clusters.json

Expected: files named like `output/resume_<cluster_slug>.tex` exist for each cluster.

    python main.py tailor vacancies/google.txt

Expected: output includes a "Best variant: <cluster_slug>" line and a similarity score.

    python main.py serve

In another terminal:

    curl -s -X POST http://localhost:8000/api/analyze \
      -H "Content-Type: application/json" \
      -d '{"job_description":"Example ML engineer role ...","use_semantic":true,"include_market_trends":false}'

Expected: JSON response where `best_variant` is a cluster slug and `category_scores` has one entry per cluster.

To verify removal of scraper dependencies:

    rg -n "linkedin_scraper|selenium|webdriver-manager" -S .

Expected: no matches.

## Validation and Acceptance

- Clustering output no longer includes irrelevant words (for example, pronouns) in `defining_skills` or `top_keywords`, and `output/vacancy_clusters.json` contains all required fields.
- Resume generation creates one LaTeX file per cluster slug under `output/`, and each file includes a header comment showing the cluster name.
- `python main.py tailor` selects a cluster slug that exists in the artifact and succeeds without relying on legacy themes.
- `/api/analyze` returns dynamic `category_scores` and categorized keywords keyed by cluster slug (plus an optional "general" group), and the frontend renders these categories without a hard-coded list.
- `deprecated/linkedin_scraper.py` and the `scrape` command are removed, and `requirements.txt` no longer includes Selenium.

## Idempotence and Recovery

Re-running the clustering command should overwrite `output/vacancy_clusters.json` safely. If cached clustering is undesired, pass `--refresh` to bypass the cache or delete `output/.vacancy_cluster_cache.json`. If cluster profiles or resume variants change, delete embedding caches such as `output/.hybrid_embeddings_cache.json` and any new cluster embedding cache to force recomputation. If generation fails mid-run, re-run `python main.py generate --clusters-artifact output/vacancy_clusters.json` to regenerate all variants.

## Artifacts and Notes

Example cluster artifact (shape only, values are illustrative):

    {
      "schema_version": 1,
      "generated_at": "2026-01-11T17:03:00Z",
      "vacancies_dir": "vacancies",
      "signature": [["google.txt", 123456789, 3456]],
      "num_clusters": 4,
      "pipeline_stats": {"cluster_selection": "requested"},
      "clusters": [
        {
          "cluster_id": 0,
          "slug": "ml-platform",
          "name": "ML Platform",
          "summary": "Production ML systems and MLOps tooling.",
          "vacancies": ["google.txt", "expedia.txt"],
          "top_keywords": ["mlops", "pipelines", "monitoring"],
          "defining_technologies": ["docker", "kubernetes"],
          "defining_skills": ["deployment", "production"],
          "keyword_counts": {"mlops": 7, "docker": 5},
          "profile_text": "ML Platform. Production ML systems and MLOps tooling. Keywords: mlops, pipelines, monitoring. Technologies: docker, kubernetes. Skills: deployment, production."
        }
      ]
    }

Example CLI output line to retain:

    Saved clustering report to: output/vacancy_clusters.json

## Interfaces and Dependencies

In `src/cluster_artifacts.py`, define:

    class ClusterArtifact(BaseModel):
        schema_version: int
        generated_at: str
        vacancies_dir: str
        signature: list[list[object]]
        num_clusters: int
        pipeline_stats: dict[str, object]
        clusters: list[ClusterArtifact.Cluster]

    class Cluster(BaseModel):
        cluster_id: int
        slug: str
        name: str
        summary: str
        vacancies: list[str]
        top_keywords: list[str]
        keyword_counts: dict[str, int]
        defining_technologies: list[str]
        defining_skills: list[str]
        profile_text: str

    def build_cluster_artifact(result: ClusterResult, vacancies_dir: Path, num_clusters: int) -> ClusterArtifact
    def load_cluster_artifact(path: str | Path) -> ClusterArtifact
    def save_cluster_artifact(path: str | Path, artifact: ClusterArtifact) -> None

In `src/vacancy_clustering.py`, add a noise filter and LLM keep/drop logic:

    def _is_noise_keyword(keyword: str) -> bool
    def cluster_and_save(self, num_clusters: int, artifact_path: str | Path) -> ClusterArtifact

Update `KeywordCategorization.CategorizedKeyword` to include a keep/drop flag (for example, `is_noise: bool`) and apply it before keyword aggregation.

In `src/cluster_matcher.py`, define:

    class ClusterMatcher:
        def __init__(self, artifact_path: str | Path, embeddings: HybridEmbeddings | None = None, cache_embeddings: bool = True) -> None
        def match(self, job_text: str) -> dict[str, object]
        async def match_async(self, job_text: str) -> dict[str, object]

Return structure must include `best_cluster`, `best_score`, and `scores` keyed by cluster slug.

In `src/resume_generator.py`, add a cluster-driven entry point:

    def generate_variants_from_clusters(source_resume_path: str | Path, output_dir: str | Path, artifact_path: str | Path, use_gpt_rewrite: bool = False) -> dict[str, Path]
    def build_theme_config_from_cluster(cluster: ClusterArtifact.Cluster) -> dict

Update `_reorder_skills_section`, `_enhance_experience_bullets`, and `_enhance_summary` to use cluster keywords instead of `primary_category`.

In `src/bullet_rewriter.py`, update `_build_bullet_prompt` and `_build_summary_prompt` to read cluster keywords from the theme config rather than `TECH_TAXONOMY`.

In `src/hybrid_keywords.py`, replace taxonomy-based categorization with a cluster-based categorizer that uses cluster keyword sets from the artifact and assigns unmatched keywords to a "general" bucket.

In `backend/schemas.py`, replace the fixed `KeywordCategory` literal with `str` and change `CategorizedKeywords` to a dynamic mapping type (for example, `dict[str, list[KeywordWithMetadata]]`). Update `AnalyzeResponse` accordingly.

In `backend/services.py`, replace `SemanticMatcher` usage with `ClusterMatcher`, and update `ResumeVariantService` to list variants based on the cluster artifact (using cluster `name` as `display_name` and `summary` as `description`).

In `frontend/src/api.ts` and `frontend/src/components/ScoreDisplay.tsx`, update types and rendering to handle dynamic category keys and display names taken from `category_scores` or a new metadata map returned by the API.

Dependencies: remove `selenium` and `webdriver-manager` from `requirements.txt`. No new third-party libraries are required.

## Plan Change Note

Plan created on 2026-01-11 to satisfy the requested cluster-driven refactor. `.agent/PLANS.md` was not present, so `PLANS.md` is used as the governing specification.
